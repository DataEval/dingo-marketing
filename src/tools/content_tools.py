"""
å†…å®¹ç”Ÿæˆç›¸å…³çš„ AI Agent å·¥å…·
"""

import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime

from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from openai import AsyncOpenAI
from loguru import logger

from src.config.settings import settings


class ContentGenerationInput(BaseModel):
    """å†…å®¹ç”Ÿæˆå·¥å…·è¾“å…¥"""
    content_type: str = Field(description="å†…å®¹ç±»å‹: blog, social, email, tutorial, documentation")
    topic: str = Field(description="å†…å®¹ä¸»é¢˜")
    target_audience: str = Field(description="ç›®æ ‡å—ä¼—")
    tone: str = Field(default="professional", description="è¯­è°ƒ: professional, casual, technical, friendly")
    length: str = Field(default="medium", description="é•¿åº¦: short, medium, long")
    language: str = Field(default="zh", description="è¯­è¨€: zh, en")
    keywords: Optional[List[str]] = Field(default=None, description="å…³é”®è¯åˆ—è¡¨")


class ContentOptimizationInput(BaseModel):
    """å†…å®¹ä¼˜åŒ–å·¥å…·è¾“å…¥"""
    content: str = Field(description="åŸå§‹å†…å®¹")
    optimization_type: str = Field(description="ä¼˜åŒ–ç±»å‹: seo, engagement, readability, technical")
    target_platform: str = Field(description="ç›®æ ‡å¹³å°: github, twitter, linkedin, blog")
    keywords: Optional[List[str]] = Field(default=None, description="SEO å…³é”®è¯")


class ContentGenerationTool(BaseTool):
    """å†…å®¹ç”Ÿæˆå·¥å…·"""
    
    name: str = "content_generation"
    description: str = "ç”Ÿæˆå„ç§ç±»å‹çš„è¥é”€å’ŒæŠ€æœ¯å†…å®¹"
    args_schema: type[BaseModel] = ContentGenerationInput
    
    def __init__(self):
        super().__init__()
        self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    def _run(self, content_type: str, topic: str, target_audience: str, 
             tone: str = "professional", length: str = "medium", 
             language: str = "zh", keywords: Optional[List[str]] = None) -> str:
        """ç”Ÿæˆå†…å®¹"""
        try:
            return asyncio.run(self._generate_content_async(
                content_type, topic, target_audience, tone, length, language, keywords
            ))
        except Exception as e:
            logger.error(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
            return f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def _generate_content_async(self, content_type: str, topic: str, 
                                    target_audience: str, tone: str, length: str, 
                                    language: str, keywords: Optional[List[str]]) -> str:
        """å¼‚æ­¥ç”Ÿæˆå†…å®¹"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_content_prompt(
                content_type, topic, target_audience, tone, length, language, keywords
            )
            
            # è°ƒç”¨ OpenAI API
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": self._get_system_prompt(content_type, language)},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=self._get_max_tokens(length)
            )
            
            content = response.choices[0].message.content
            
            # åå¤„ç†
            processed_content = self._post_process_content(content, content_type, keywords)
            
            return f"âœ… {content_type} å†…å®¹ç”Ÿæˆå®Œæˆ:\n\n{processed_content}"
            
        except Exception as e:
            return f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _build_content_prompt(self, content_type: str, topic: str, target_audience: str,
                            tone: str, length: str, language: str, 
                            keywords: Optional[List[str]]) -> str:
        """æ„å»ºå†…å®¹ç”Ÿæˆæç¤ºè¯"""
        
        # åŸºç¡€æç¤ºè¯æ¨¡æ¿
        base_prompts = {
            "blog": "å†™ä¸€ç¯‡å…³äº {topic} çš„åšå®¢æ–‡ç« ",
            "social": "åˆ›å»ºå…³äº {topic} çš„ç¤¾äº¤åª’ä½“å†…å®¹",
            "email": "å†™ä¸€å°å…³äº {topic} çš„è¥é”€é‚®ä»¶",
            "tutorial": "åˆ›å»ºå…³äº {topic} çš„æ•™ç¨‹å†…å®¹",
            "documentation": "ç¼–å†™å…³äº {topic} çš„æŠ€æœ¯æ–‡æ¡£"
        }
        
        # è¯­è°ƒæè¿°
        tone_descriptions = {
            "professional": "ä¸“ä¸šã€æ­£å¼",
            "casual": "è½»æ¾ã€éšæ„",
            "technical": "æŠ€æœ¯æ€§ã€è¯¦ç»†",
            "friendly": "å‹å¥½ã€äº²åˆ‡"
        }
        
        # é•¿åº¦æè¿°
        length_descriptions = {
            "short": "ç®€çŸ­ï¼ˆ200-300å­—ï¼‰",
            "medium": "ä¸­ç­‰é•¿åº¦ï¼ˆ500-800å­—ï¼‰",
            "long": "è¯¦ç»†ï¼ˆ1000-1500å­—ï¼‰"
        }
        
        prompt = base_prompts.get(content_type, "åˆ›å»ºå…³äº {topic} çš„å†…å®¹").format(topic=topic)
        
        prompt += f"""

ç›®æ ‡å—ä¼—: {target_audience}
è¯­è°ƒé£æ ¼: {tone_descriptions.get(tone, tone)}
å†…å®¹é•¿åº¦: {length_descriptions.get(length, length)}
è¯­è¨€: {'ä¸­æ–‡' if language == 'zh' else 'è‹±æ–‡'}
"""
        
        if keywords:
            prompt += f"\nå…³é”®è¯: {', '.join(keywords)}"
        
        # æ·»åŠ ç‰¹å®šç±»å‹çš„è¦æ±‚
        if content_type == "blog":
            prompt += "\n\nè¦æ±‚:\n- åŒ…å«å¼•äººå…¥èƒœçš„æ ‡é¢˜\n- ç»“æ„æ¸…æ™°ï¼Œæœ‰å°æ ‡é¢˜\n- åŒ…å«å®ç”¨çš„å»ºè®®æˆ–è§è§£\n- é€‚åˆ SEO ä¼˜åŒ–"
        elif content_type == "social":
            prompt += "\n\nè¦æ±‚:\n- ç®€æ´æœ‰åŠ›\n- åŒ…å«ç›¸å…³è¯é¢˜æ ‡ç­¾\n- é¼“åŠ±äº’åŠ¨\n- é€‚åˆç¤¾äº¤åª’ä½“ä¼ æ’­"
        elif content_type == "email":
            prompt += "\n\nè¦æ±‚:\n- æœ‰å¸å¼•åŠ›çš„ä¸»é¢˜è¡Œ\n- ä¸ªæ€§åŒ–çš„å¼€å¤´\n- æ¸…æ™°çš„è¡ŒåŠ¨å·å¬\n- ä¸“ä¸šçš„ç»“å°¾"
        elif content_type == "tutorial":
            prompt += "\n\nè¦æ±‚:\n- æ­¥éª¤æ¸…æ™°\n- åŒ…å«ä»£ç ç¤ºä¾‹ï¼ˆå¦‚é€‚ç”¨ï¼‰\n- æ˜“äºç†è§£\n- åŒ…å«å¸¸è§é—®é¢˜è§£ç­”"
        elif content_type == "documentation":
            prompt += "\n\nè¦æ±‚:\n- ç»“æ„åŒ–çš„æ ¼å¼\n- è¯¦ç»†çš„è¯´æ˜\n- åŒ…å«ç¤ºä¾‹\n- æŠ€æœ¯å‡†ç¡®æ€§"
        
        return prompt
    
    def _get_system_prompt(self, content_type: str, language: str) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        if language == "zh":
            base_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ï¼Œæ“…é•¿åˆ›å»ºé«˜è´¨é‡çš„è¥é”€å’ŒæŠ€æœ¯å†…å®¹ã€‚"
        else:
            base_prompt = "You are a professional content creation expert, skilled at creating high-quality marketing and technical content."
        
        type_specific = {
            "blog": "ä¸“æ³¨äºåˆ›å»ºæœ‰ä»·å€¼ã€æœ‰è§è§£çš„åšå®¢å†…å®¹",
            "social": "ä¸“æ³¨äºåˆ›å»ºå¼•äººå…¥èƒœçš„ç¤¾äº¤åª’ä½“å†…å®¹",
            "email": "ä¸“æ³¨äºåˆ›å»ºæœ‰æ•ˆçš„è¥é”€é‚®ä»¶",
            "tutorial": "ä¸“æ³¨äºåˆ›å»ºæ¸…æ™°æ˜“æ‡‚çš„æ•™ç¨‹",
            "documentation": "ä¸“æ³¨äºåˆ›å»ºå‡†ç¡®è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£"
        }
        
        return f"{base_prompt} {type_specific.get(content_type, '')}"
    
    def _get_max_tokens(self, length: str) -> int:
        """æ ¹æ®é•¿åº¦è·å–æœ€å¤§ token æ•°"""
        token_limits = {
            "short": 500,
            "medium": 1000,
            "long": 2000
        }
        return token_limits.get(length, 1000)
    
    def _post_process_content(self, content: str, content_type: str, 
                            keywords: Optional[List[str]]) -> str:
        """åå¤„ç†å†…å®¹"""
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # æ ¹æ®å†…å®¹ç±»å‹æ·»åŠ ç‰¹å®šæ ¼å¼
        if content_type == "social":
            # ç¡®ä¿ç¤¾äº¤åª’ä½“å†…å®¹æœ‰é€‚å½“çš„æ ¼å¼
            if not content.startswith("#"):
                content = f"ğŸš€ {content}"
        elif content_type == "email":
            # ç¡®ä¿é‚®ä»¶æœ‰é€‚å½“çš„æ ¼å¼
            if "ä¸»é¢˜:" not in content and "Subject:" not in content:
                lines = content.split('\n')
                content = f"ä¸»é¢˜: {lines[0]}\n\n" + '\n'.join(lines[1:])
        
        # æ·»åŠ å…ƒæ•°æ®
        metadata = f"\n\n---\nç”Ÿæˆæ—¶é—´: {timestamp}\nå†…å®¹ç±»å‹: {content_type}"
        if keywords:
            metadata += f"\nå…³é”®è¯: {', '.join(keywords)}"
        
        return content + metadata


class ContentOptimizationTool(BaseTool):
    """å†…å®¹ä¼˜åŒ–å·¥å…·"""
    
    name: str = "content_optimization"
    description: str = "ä¼˜åŒ–ç°æœ‰å†…å®¹ä»¥æé«˜æ•ˆæœ"
    args_schema: type[BaseModel] = ContentOptimizationInput
    
    def __init__(self):
        super().__init__()
        self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    def _run(self, content: str, optimization_type: str, target_platform: str,
             keywords: Optional[List[str]] = None) -> str:
        """ä¼˜åŒ–å†…å®¹"""
        try:
            return asyncio.run(self._optimize_content_async(
                content, optimization_type, target_platform, keywords
            ))
        except Exception as e:
            logger.error(f"å†…å®¹ä¼˜åŒ–å¤±è´¥: {e}")
            return f"å†…å®¹ä¼˜åŒ–å¤±è´¥: {str(e)}"
    
    async def _optimize_content_async(self, content: str, optimization_type: str,
                                    target_platform: str, keywords: Optional[List[str]]) -> str:
        """å¼‚æ­¥ä¼˜åŒ–å†…å®¹"""
        try:
            # æ„å»ºä¼˜åŒ–æç¤ºè¯
            prompt = self._build_optimization_prompt(
                content, optimization_type, target_platform, keywords
            )
            
            # è°ƒç”¨ OpenAI API
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": self._get_optimization_system_prompt(optimization_type)},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=2000
            )
            
            optimized_content = response.choices[0].message.content
            
            return f"âœ… å†…å®¹ä¼˜åŒ–å®Œæˆ ({optimization_type}):\n\n{optimized_content}"
            
        except Exception as e:
            return f"å†…å®¹ä¼˜åŒ–å¤±è´¥: {str(e)}"
    
    def _build_optimization_prompt(self, content: str, optimization_type: str,
                                 target_platform: str, keywords: Optional[List[str]]) -> str:
        """æ„å»ºä¼˜åŒ–æç¤ºè¯"""
        
        optimization_instructions = {
            "seo": "ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ä»¥æé«˜æœç´¢å¼•æ“æ’å",
            "engagement": "ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ä»¥æé«˜ç”¨æˆ·å‚ä¸åº¦",
            "readability": "ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ä»¥æé«˜å¯è¯»æ€§",
            "technical": "ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ä»¥æé«˜æŠ€æœ¯å‡†ç¡®æ€§"
        }
        
        platform_requirements = {
            "github": "é€‚åˆ GitHub README æˆ–æ–‡æ¡£æ ¼å¼",
            "twitter": "é€‚åˆ Twitter çš„å­—ç¬¦é™åˆ¶å’Œæ ¼å¼",
            "linkedin": "é€‚åˆ LinkedIn çš„ä¸“ä¸šé£æ ¼",
            "blog": "é€‚åˆåšå®¢æ–‡ç« çš„æ ¼å¼å’Œç»“æ„"
        }
        
        prompt = f"{optimization_instructions.get(optimization_type, 'ä¼˜åŒ–ä»¥ä¸‹å†…å®¹')}ï¼Œ"
        prompt += f"ä½¿å…¶{platform_requirements.get(target_platform, 'æ›´é€‚åˆç›®æ ‡å¹³å°')}ã€‚\n\n"
        
        if keywords:
            prompt += f"é‡ç‚¹å…³é”®è¯: {', '.join(keywords)}\n\n"
        
        prompt += f"åŸå§‹å†…å®¹:\n{content}\n\n"
        
        # æ·»åŠ å…·ä½“ä¼˜åŒ–è¦æ±‚
        if optimization_type == "seo":
            prompt += "ä¼˜åŒ–è¦æ±‚:\n- è‡ªç„¶åœ°èå…¥å…³é”®è¯\n- ä¼˜åŒ–æ ‡é¢˜å’Œå°æ ‡é¢˜\n- æ”¹å–„å†…å®¹ç»“æ„\n- å¢åŠ ç›¸å…³æ€§"
        elif optimization_type == "engagement":
            prompt += "ä¼˜åŒ–è¦æ±‚:\n- å¢åŠ äº’åŠ¨å…ƒç´ \n- ä½¿ç”¨æ›´å¸å¼•äººçš„è¯­è¨€\n- æ·»åŠ è¡ŒåŠ¨å·å¬\n- æé«˜æƒ…æ„Ÿå…±é¸£"
        elif optimization_type == "readability":
            prompt += "ä¼˜åŒ–è¦æ±‚:\n- ç®€åŒ–å¤æ‚å¥å­\n- æ”¹å–„æ®µè½ç»“æ„\n- ä½¿ç”¨æ›´æ¸…æ™°çš„è¡¨è¾¾\n- å¢åŠ å¯è¯»æ€§"
        elif optimization_type == "technical":
            prompt += "ä¼˜åŒ–è¦æ±‚:\n- ç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§\n- æ·»åŠ å¿…è¦çš„æŠ€æœ¯ç»†èŠ‚\n- æ”¹å–„ä»£ç ç¤ºä¾‹\n- å¢åŠ æŠ€æœ¯æ·±åº¦"
        
        return prompt
    
    def _get_optimization_system_prompt(self, optimization_type: str) -> str:
        """è·å–ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯"""
        base_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹ä¼˜åŒ–ä¸“å®¶ã€‚"
        
        type_specific = {
            "seo": "ä¸“æ³¨äºæœç´¢å¼•æ“ä¼˜åŒ–ï¼Œæé«˜å†…å®¹çš„æœç´¢æ’åã€‚",
            "engagement": "ä¸“æ³¨äºæé«˜ç”¨æˆ·å‚ä¸åº¦å’Œäº’åŠ¨æ€§ã€‚",
            "readability": "ä¸“æ³¨äºæé«˜å†…å®¹çš„å¯è¯»æ€§å’Œç†è§£æ€§ã€‚",
            "technical": "ä¸“æ³¨äºæé«˜æŠ€æœ¯å†…å®¹çš„å‡†ç¡®æ€§å’Œæ·±åº¦ã€‚"
        }
        
        return f"{base_prompt} {type_specific.get(optimization_type, '')}"


class ContentAnalysisTool(BaseTool):
    """å†…å®¹åˆ†æå·¥å…·"""
    
    name: str = "content_analysis"
    description: str = "åˆ†æå†…å®¹çš„è´¨é‡ã€SEO æ•ˆæœå’Œæ”¹è¿›å»ºè®®"
    args_schema: type[BaseModel] = ContentOptimizationInput
    
    def _run(self, content: str, optimization_type: str = "general", 
             target_platform: str = "general", keywords: Optional[List[str]] = None) -> str:
        """åˆ†æå†…å®¹"""
        try:
            analysis_result = self._analyze_content(content, keywords)
            return f"âœ… å†…å®¹åˆ†æå®Œæˆ:\n\n{analysis_result}"
        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
    
    def _analyze_content(self, content: str, keywords: Optional[List[str]]) -> str:
        """åˆ†æå†…å®¹è´¨é‡"""
        analysis = []
        
        # åŸºæœ¬ç»Ÿè®¡
        word_count = len(content.split())
        char_count = len(content)
        paragraph_count = len([p for p in content.split('\n\n') if p.strip()])
        
        analysis.append(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        analysis.append(f"- å­—æ•°: {word_count}")
        analysis.append(f"- å­—ç¬¦æ•°: {char_count}")
        analysis.append(f"- æ®µè½æ•°: {paragraph_count}")
        
        # å¯è¯»æ€§åˆ†æ
        avg_words_per_paragraph = word_count / max(paragraph_count, 1)
        analysis.append(f"\nğŸ“– å¯è¯»æ€§åˆ†æ:")
        analysis.append(f"- å¹³å‡æ¯æ®µå­—æ•°: {avg_words_per_paragraph:.1f}")
        
        if avg_words_per_paragraph > 100:
            analysis.append("- å»ºè®®: æ®µè½è¿‡é•¿ï¼Œå»ºè®®åˆ†å‰²")
        elif avg_words_per_paragraph < 20:
            analysis.append("- å»ºè®®: æ®µè½è¿‡çŸ­ï¼Œå¯ä»¥é€‚å½“åˆå¹¶")
        else:
            analysis.append("- æ®µè½é•¿åº¦é€‚ä¸­")
        
        # å…³é”®è¯åˆ†æ
        if keywords:
            analysis.append(f"\nğŸ” å…³é”®è¯åˆ†æ:")
            for keyword in keywords:
                count = content.lower().count(keyword.lower())
                density = (count * len(keyword.split())) / word_count * 100
                analysis.append(f"- '{keyword}': å‡ºç° {count} æ¬¡, å¯†åº¦ {density:.1f}%")
                
                if density < 1:
                    analysis.append(f"  å»ºè®®: å¢åŠ  '{keyword}' çš„ä½¿ç”¨é¢‘ç‡")
                elif density > 3:
                    analysis.append(f"  å»ºè®®: å‡å°‘ '{keyword}' çš„ä½¿ç”¨é¢‘ç‡")
        
        # ç»“æ„åˆ†æ
        analysis.append(f"\nğŸ—ï¸ ç»“æ„åˆ†æ:")
        has_headings = any(line.startswith('#') for line in content.split('\n'))
        has_lists = any(line.strip().startswith(('-', '*', '1.')) for line in content.split('\n'))
        
        analysis.append(f"- åŒ…å«æ ‡é¢˜: {'æ˜¯' if has_headings else 'å¦'}")
        analysis.append(f"- åŒ…å«åˆ—è¡¨: {'æ˜¯' if has_lists else 'å¦'}")
        
        if not has_headings:
            analysis.append("- å»ºè®®: æ·»åŠ æ ‡é¢˜æ¥æ”¹å–„ç»“æ„")
        if not has_lists and word_count > 200:
            analysis.append("- å»ºè®®: ä½¿ç”¨åˆ—è¡¨æ¥æé«˜å¯è¯»æ€§")
        
        # æ€»ä½“è¯„åˆ†
        score = 0
        if 200 <= word_count <= 1500:
            score += 25
        if 20 <= avg_words_per_paragraph <= 100:
            score += 25
        if has_headings:
            score += 25
        if has_lists:
            score += 25
        
        analysis.append(f"\nâ­ æ€»ä½“è¯„åˆ†: {score}/100")
        
        if score >= 80:
            analysis.append("- å†…å®¹è´¨é‡ä¼˜ç§€")
        elif score >= 60:
            analysis.append("- å†…å®¹è´¨é‡è‰¯å¥½ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
        else:
            analysis.append("- å†…å®¹éœ€è¦æ˜¾è‘—æ”¹è¿›")
        
        return '\n'.join(analysis) 